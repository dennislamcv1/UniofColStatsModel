{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4: Peer Reviewed Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline:\n",
    "The objectives for this assignment:\n",
    "\n",
    "1. Understand mean intervals and Prediction Intervals through read data applications and visualizations.\n",
    "2. Observe how CIs and PIs change on different data sets.\n",
    "3. Observe and analyze interval curvature.\n",
    "4. Apply understanding of causation to experimental and observational studies.\n",
    "\n",
    "General tips:\n",
    "\n",
    "1. Read the questions carefully to understand what is being asked.\n",
    "2. This work will be reviewed by another human, so make sure that you are clear and concise in what your explanations and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered S3 methods overwritten by 'ggplot2':\n",
      "  method         from \n",
      "  [.quosures     rlang\n",
      "  c.quosures     rlang\n",
      "  print.quosures rlang\n",
      "Registered S3 method overwritten by 'rvest':\n",
      "  method            from\n",
      "  read_xml.response xml2\n",
      "-- Attaching packages --------------------------------------- tidyverse 1.2.1 --\n",
      "v ggplot2 3.1.1     v purrr   0.3.2\n",
      "v tibble  3.0.4     v dplyr   1.0.2\n",
      "v tidyr   1.1.2     v stringr 1.4.0\n",
      "v readr   1.3.1     v forcats 0.4.0\n",
      "Warning message:\n",
      "\"package 'tibble' was built under R version 3.6.3\"Warning message:\n",
      "\"package 'tidyr' was built under R version 3.6.3\"Warning message:\n",
      "\"package 'dplyr' was built under R version 3.6.3\"-- Conflicts ------------------------------------------ tidyverse_conflicts() --\n",
      "x dplyr::filter() masks stats::filter()\n",
      "x dplyr::lag()    masks stats::lag()\n"
     ]
    }
   ],
   "source": [
    "# This cell loads the necesary libraries for this assignment\n",
    "library(tidyverse)\n",
    "library(ggplot2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Interpreting Intervals\n",
    "\n",
    "For this problem, we're going to practice creating and interpreting Confidence (Mean) Intervals and Prediction Intervals. To do so, we're going to use data in U.S. State Wine Consumption (millions of liters) and Population (millions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. (a) Initial Inspections\n",
    "\n",
    "Load in the data and create a scatterplot with `population` on the x-axis and `totWine` on the y-axis. For fun, set the color of the point to be `#CFB87C`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>State</th><th scope=col>pcWine</th><th scope=col>pop</th><th scope=col>totWine</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Alabama   </td><td> 6.0      </td><td> 4.829479 </td><td> 28.976874</td></tr>\n",
       "\t<tr><td>Alaska    </td><td>10.9      </td><td> 0.736879 </td><td>  8.031981</td></tr>\n",
       "\t<tr><td>Arizona   </td><td> 9.7      </td><td> 6.624617 </td><td> 64.258785</td></tr>\n",
       "\t<tr><td>Arkansas  </td><td> 4.2      </td><td> 2.958663 </td><td> 12.426385</td></tr>\n",
       "\t<tr><td>California</td><td>14.0      </td><td>38.335203 </td><td>536.692842</td></tr>\n",
       "\t<tr><td>Colorado  </td><td> 8.7      </td><td> 5.267603 </td><td> 45.828146</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       " State & pcWine & pop & totWine\\\\\n",
       "\\hline\n",
       "\t Alabama    &  6.0       &  4.829479  &  28.976874\\\\\n",
       "\t Alaska     & 10.9       &  0.736879  &   8.031981\\\\\n",
       "\t Arizona    &  9.7       &  6.624617  &  64.258785\\\\\n",
       "\t Arkansas   &  4.2       &  2.958663  &  12.426385\\\\\n",
       "\t California & 14.0       & 38.335203  & 536.692842\\\\\n",
       "\t Colorado   &  8.7       &  5.267603  &  45.828146\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| State | pcWine | pop | totWine |\n",
       "|---|---|---|---|\n",
       "| Alabama    |  6.0       |  4.829479  |  28.976874 |\n",
       "| Alaska     | 10.9       |  0.736879  |   8.031981 |\n",
       "| Arizona    |  9.7       |  6.624617  |  64.258785 |\n",
       "| Arkansas   |  4.2       |  2.958663  |  12.426385 |\n",
       "| California | 14.0       | 38.335203  | 536.692842 |\n",
       "| Colorado   |  8.7       |  5.267603  |  45.828146 |\n",
       "\n"
      ],
      "text/plain": [
       "  State      pcWine pop       totWine   \n",
       "1 Alabama     6.0    4.829479  28.976874\n",
       "2 Alaska     10.9    0.736879   8.031981\n",
       "3 Arizona     9.7    6.624617  64.258785\n",
       "4 Arkansas    4.2    2.958663  12.426385\n",
       "5 California 14.0   38.335203 536.692842\n",
       "6 Colorado    8.7    5.267603  45.828146"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the data\n",
    "wine.data = read.csv(\"wine_state_2013.csv\")\n",
    "head(wine.data)\n",
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. (b) Confidence Intervals\n",
    "\n",
    "Fit a linear regression with `totWine` as the response and `pop` as the predictor. Add the regression line to your scatterplot. For fun, set its color to gold with `col=#CFB87C`. Add the $90\\%$ Confidence Interval for the regression line to the plot.\n",
    "\n",
    "Then choose a single point-value population and display the upper and lower values for the Confidence Interval at that point. In words, explain what this interval means for that data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. (c) Prediction Intervals\n",
    "\n",
    "Using the same `pop` point-value as in **1.b**, plot the prediction interval end points. In words, explain what this interval means for that data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. (d) Some \"Consequences\" of Linear Regression\n",
    "\n",
    "As you've probably gathered by now, there is a lot of math that goes into fitting linear models. It's important that you're exposed to these underlying systems and build an intuition for how certain processes work. However, some of the math can be a bit too... tedious for us to make you go through on your own. Below are a list of \"consequences\" of linear regression, things that are mathematically true because of the assumptions and formulations of the linear model (let $\\widehat\\varepsilon_i$ be the residuals of the regression model):\n",
    "\n",
    "1. $\\sum \\widehat\\varepsilon_i = 0$ : The sum of residuals is 0.\n",
    "2. $\\sum \\widehat\\varepsilon_i^2$ is as small as it can be.\n",
    "3. $\\sum x_i \\widehat\\varepsilon_i = 0$\n",
    "4. $\\sum \\hat{y}_i \\widehat\\varepsilon_i = 0$ : The Residuals are orthogonal to the fitted values.\n",
    "5. The Regression Line always goes through $(\\bar{x}, \\bar{y})$.\n",
    "\n",
    "Check that your regression model confirms the \"consequences\" $1,3,4$ and $5$. For consequence $2$, give a logical reason on why this formulation makes sense.\n",
    "\n",
    "**Note: even if your data agrees with these claims, that does not prove them as fact. For best practice, try to prove these facts yourself!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Explanation\n",
    "\n",
    "<img src=\"xkcd_correlation.png\" style=\"width:600px;\"/>\n",
    "\n",
    "Image Source: https://xkcd.com/552/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did our wine drinking data come from an experiment or an observational study? Do you think we can infer causation between population and the amount of wine drank from these data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Even More Intervals!\n",
    "\n",
    "We're almost done! There is just a few more details about Confidence Intervals and Perdiction Intervals which we want to go over. How does changing the data affect the confidence interval? That's a hard question to answer with a single dataset, so let's simulate a bunch of different datasets and see what they intervals they produce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. (a) Visualize the data\n",
    "\n",
    "The code cell below generates 20 data points from two different normal distributions. Finish the code by fitting a linear model to the data and plotting the results with ggplot, with Confidence Intervals for the mean and Prediction Intervals included. \n",
    "\n",
    "Experiment with different means and variances. Does changing these values affect the CI or PI?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>x</th><th scope=col>y</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td> -5.474091 </td><td>-11.1908617</td></tr>\n",
       "\t<tr><td> -8.652467 </td><td>-11.5309770</td></tr>\n",
       "\t<tr><td> -5.340401 </td><td> -7.3474393</td></tr>\n",
       "\t<tr><td> -5.455141 </td><td> -0.8683876</td></tr>\n",
       "\t<tr><td> -7.170717 </td><td>-12.9125020</td></tr>\n",
       "\t<tr><td>-11.079900 </td><td>-15.1237204</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " x & y\\\\\n",
       "\\hline\n",
       "\t  -5.474091  & -11.1908617\\\\\n",
       "\t  -8.652467  & -11.5309770\\\\\n",
       "\t  -5.340401  &  -7.3474393\\\\\n",
       "\t  -5.455141  &  -0.8683876\\\\\n",
       "\t  -7.170717  & -12.9125020\\\\\n",
       "\t -11.079900  & -15.1237204\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| x | y |\n",
       "|---|---|\n",
       "|  -5.474091  | -11.1908617 |\n",
       "|  -8.652467  | -11.5309770 |\n",
       "|  -5.340401  |  -7.3474393 |\n",
       "|  -5.455141  |  -0.8683876 |\n",
       "|  -7.170717  | -12.9125020 |\n",
       "| -11.079900  | -15.1237204 |\n",
       "\n"
      ],
      "text/plain": [
       "  x          y          \n",
       "1  -5.474091 -11.1908617\n",
       "2  -8.652467 -11.5309770\n",
       "3  -5.340401  -7.3474393\n",
       "4  -5.455141  -0.8683876\n",
       "5  -7.170717 -12.9125020\n",
       "6 -11.079900 -15.1237204"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gen_data <- function(mu1, mu2, var1, var2){\n",
    "    # Function to generate 20 data points from 2 different normal distributions.\n",
    "    x.1 = rnorm(10, mu1, 2)\n",
    "    x.2 = rnorm(10, mu2, 2)\n",
    "    y.1 = 2 + 2*x.1 + rnorm(10, 0, var1)\n",
    "    y.2 = 2 + 2*x.2 + rnorm(10, 0, var2)\n",
    "\n",
    "    df = data.frame(x=c(x.1, x.2), y=c(y.1, y.2))\n",
    "    return(df)\n",
    "}\n",
    "\n",
    "set.seed(0)\n",
    "head(gen_data(-8, 8, 10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. (b) The Smallest Interval\n",
    "\n",
    "Recall that the Confidence (Mean) Interval, when the predictor value is $x_k$, is defined as:\n",
    "\n",
    "$$\n",
    "\\hat{y}_h \\pm t_{\\alpha/2,n-2} \\sqrt{MSE \\times \\Big( \\dfrac{1}{n} + \\dfrac{(x_k - \\bar{x})^2}{\\sum(x_i - \\bar{x})} \\Big)}\n",
    "$$\n",
    "\n",
    "where $\\hat{y}_h$ is the fitted response for predictor value $x_h$, $t_{\\alpha/2,n-2}$ is the t-value with $n-2$ degrees of freedom and $MSE \\times \\Big( \\dfrac{1}{n} + \\dfrac{(x_h - \\bar{x})^2}{\\sum(x_i - \\bar{x})} \\Big)$ is the standard error of the fit.\n",
    "\n",
    "From the above equation, what value of $x_k$ would result in the CI with the shortest width? Does this match up with the simulated data? Can you give an intuitive reason for why this occurs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. (c) Interviewing the Intervals\n",
    "\n",
    "Recall that the Prediction Interval, when the predictor value is $x_k$, is defined as:\n",
    "\n",
    "$$\n",
    "\\hat{y}_h \\pm t_{\\alpha/2,n-2} \\sqrt{MSE \\Big( 1 + \\dfrac{1}{n} + \\dfrac{(x_k - \\bar{x})^2}{\\sum(x_i - \\bar{x})} \\Big)}\n",
    "$$\n",
    "\n",
    "Does the \"width\" of the Prediction Interval change at different population values? Explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6fa9a9a07d81c972",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Problem 4: Causality\n",
    "\n",
    "**Please answer the following three questions. Each answer should be clearly labeled, and a few sentences to a paragraph long.**\n",
    "\n",
    "1. In your own words, describe the fundamental problem of causal inference. How is this problem related to the counterfactual definition of causality?\n",
    "\n",
    "\n",
    "2. Describe the use of \"close substitutes\" as a solution to the fundamental problem of causal inference. How does this solve the problem?\n",
    "\n",
    "\n",
    "3. What is the difference between a *deterministic* theory of causality and a *probabilistic* theory of causality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5546ee8d83a3daaa",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "1. \n",
    "\n",
    "2. \n",
    "\n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bb8400e77655dcfe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Problem 5: Causal inference and ethics\n",
    "\n",
    "How we think about causality, and the statistical models that we use to learn about causal relationships, have ethical implications. The goal of this problem is to invite you to think through some of those issues and implications.\n",
    "\n",
    "Statisticians, data scientists, researchers, etc., are not in agreement on the best ways to study and analyze important social problems, such as racial discrimination in the criminal justice system. Lily Hu, a PhD candidate in applied math and philosophy at Harvard, [wrote](https://lpeproject.org/blog/law-liberation-and-causal-inference/) that disagreements about how to best study these problems \"well illustrate how the nuts and bolts of causal inference...about the quantitative ventures to compute 'effects of race'...feature a slurry of theoretical, empirical, and normative reasoning that is often displaced into debates about purely technical matters in methodology.\"\n",
    "\n",
    "Here are some resources that enter into or comment on this debate:\n",
    "\n",
    "1. [Statistical controversy on estimating racial bias in the criminal justice system](https://statmodeling.stat.columbia.edu/2020/07/06/statistical-controversy-on-racial-bias-in-the-criminal-justice-system/)\n",
    "\n",
    "2. [Can Racial Bias in Policing Be Credibly Estimated Using Data Contaminated by Post-Treatment Selection?](https://dcknox.github.io/files/KnoxLoweMummolo_PostTreatmentSelectionPolicing.pdf)\n",
    "\n",
    "3. [A Causal Framework for Observational Studies of Discrimination](https://5harad.com/papers/post-treatment-bias.pdf)\n",
    "\n",
    "**Please read Lily Hu's [blog post](https://lpeproject.org/blog/law-liberation-and-causal-inference/) and Andrew Gelman's blog post [\"Statistical controversy on estimating racial bias in the criminal justice system\"](https://statmodeling.stat.columbia.edu/2020/07/06/statistical-controversy-on-racial-bias-in-the-criminal-justice-system/) (and feel free to continue on with the other two papers!) to familiarize yourself with some of the issues in this debate. Then, write a short essay (300-500 words) summarizing this debate. Some important items to consider:**\n",
    "\n",
    "1. How does the \"fundamental problem of causal inference\" play out in these discussions?\n",
    "\n",
    "\n",
    "2. What are some \"possible distortionary effect[s] of using arrest data from administrative police records to measure causal effects of race\"?\n",
    "\n",
    "\n",
    "3. What role do assumptions (both statistical and otherwise) play in this debate? To what extent are assumptions made by different researchers falsifiable?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-afd1a2921894e89f",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
