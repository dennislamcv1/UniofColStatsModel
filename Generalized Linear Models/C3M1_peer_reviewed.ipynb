{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C3M1: Peer Reviewed Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline:\n",
    "The objectives for this assignment:\n",
    "\n",
    "1. Apply Binomial regression methods to real data.\n",
    "2. Understand how to analyze and interpret binomial regression models.\n",
    "3. Flex our math skills by determining whether certain distributions are members of the exponential family.\n",
    "\n",
    "General tips:\n",
    "\n",
    "1. Read the questions carefully to understand what is being asked.\n",
    "2. This work will be reviewed by another human, so make sure that you are clear and concise in what your explanations and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "── \u001b[1mAttaching packages\u001b[22m ─────────────────────────────────────── tidyverse 1.3.0 ──\n",
      "\n",
      "\u001b[32m✔\u001b[39m \u001b[34mggplot2\u001b[39m 3.3.0     \u001b[32m✔\u001b[39m \u001b[34mpurrr  \u001b[39m 0.3.4\n",
      "\u001b[32m✔\u001b[39m \u001b[34mtibble \u001b[39m 3.2.1     \u001b[32m✔\u001b[39m \u001b[34mdplyr  \u001b[39m 1.1.2\n",
      "\u001b[32m✔\u001b[39m \u001b[34mtidyr  \u001b[39m 1.0.2     \u001b[32m✔\u001b[39m \u001b[34mstringr\u001b[39m 1.4.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mreadr  \u001b[39m 1.3.1     \u001b[32m✔\u001b[39m \u001b[34mforcats\u001b[39m 0.5.0\n",
      "\n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load required libraries\n",
    "library(tidyverse)\n",
    "library(dplyr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Binomial (Logistic) Regression\n",
    "\n",
    "The National Institute of Diabetes and Digestive and Kidney Diseases conducted a study of 768 adult female Pima Indians living near Phoenix, AZ. The purpose of the study was to investigate the factors related to diabetes. \n",
    "\n",
    "*Before we analyze these data, we should note that some have raised ethical issues with its collection and popularity in the statistics and data science community. We should think seriously about these concerns. For example, Maya Iskandarani wrote a brief [piece](https://researchblog.duke.edu/2016/10/24/diabetes-and-privacy-meet-big-data/) on consent and privacy concerns raised by this dataset. After you familarize yourself with the data, we'll then turn to these ethical concerns.*\n",
    "\n",
    "\n",
    "First, we'll use these data to get some practice with GLM and Logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 6 × 9</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>pregnant</th><th scope=col>glucose</th><th scope=col>diastolic</th><th scope=col>triceps</th><th scope=col>insulin</th><th scope=col>bmi</th><th scope=col>diabetes</th><th scope=col>age</th><th scope=col>test</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>6</td><td>148</td><td>72</td><td>35</td><td>  0</td><td>33.6</td><td>0.627</td><td>50</td><td>1</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>1</td><td> 85</td><td>66</td><td>29</td><td>  0</td><td>26.6</td><td>0.351</td><td>31</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>8</td><td>183</td><td>64</td><td> 0</td><td>  0</td><td>23.3</td><td>0.672</td><td>32</td><td>1</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>1</td><td> 89</td><td>66</td><td>23</td><td> 94</td><td>28.1</td><td>0.167</td><td>21</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>0</td><td>137</td><td>40</td><td>35</td><td>168</td><td>43.1</td><td>2.288</td><td>33</td><td>1</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>5</td><td>116</td><td>74</td><td> 0</td><td>  0</td><td>25.6</td><td>0.201</td><td>30</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 9\n",
       "\\begin{tabular}{r|lllllllll}\n",
       "  & pregnant & glucose & diastolic & triceps & insulin & bmi & diabetes & age & test\\\\\n",
       "  & <int> & <int> & <int> & <int> & <int> & <dbl> & <dbl> & <int> & <int>\\\\\n",
       "\\hline\n",
       "\t1 & 6 & 148 & 72 & 35 &   0 & 33.6 & 0.627 & 50 & 1\\\\\n",
       "\t2 & 1 &  85 & 66 & 29 &   0 & 26.6 & 0.351 & 31 & 0\\\\\n",
       "\t3 & 8 & 183 & 64 &  0 &   0 & 23.3 & 0.672 & 32 & 1\\\\\n",
       "\t4 & 1 &  89 & 66 & 23 &  94 & 28.1 & 0.167 & 21 & 0\\\\\n",
       "\t5 & 0 & 137 & 40 & 35 & 168 & 43.1 & 2.288 & 33 & 1\\\\\n",
       "\t6 & 5 & 116 & 74 &  0 &   0 & 25.6 & 0.201 & 30 & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 9\n",
       "\n",
       "| <!--/--> | pregnant &lt;int&gt; | glucose &lt;int&gt; | diastolic &lt;int&gt; | triceps &lt;int&gt; | insulin &lt;int&gt; | bmi &lt;dbl&gt; | diabetes &lt;dbl&gt; | age &lt;int&gt; | test &lt;int&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | 6 | 148 | 72 | 35 |   0 | 33.6 | 0.627 | 50 | 1 |\n",
       "| 2 | 1 |  85 | 66 | 29 |   0 | 26.6 | 0.351 | 31 | 0 |\n",
       "| 3 | 8 | 183 | 64 |  0 |   0 | 23.3 | 0.672 | 32 | 1 |\n",
       "| 4 | 1 |  89 | 66 | 23 |  94 | 28.1 | 0.167 | 21 | 0 |\n",
       "| 5 | 0 | 137 | 40 | 35 | 168 | 43.1 | 2.288 | 33 | 1 |\n",
       "| 6 | 5 | 116 | 74 |  0 |   0 | 25.6 | 0.201 | 30 | 0 |\n",
       "\n"
      ],
      "text/plain": [
       "  pregnant glucose diastolic triceps insulin bmi  diabetes age test\n",
       "1 6        148     72        35        0     33.6 0.627    50  1   \n",
       "2 1         85     66        29        0     26.6 0.351    31  0   \n",
       "3 8        183     64         0        0     23.3 0.672    32  1   \n",
       "4 1         89     66        23       94     28.1 0.167    21  0   \n",
       "5 0        137     40        35      168     43.1 2.288    33  1   \n",
       "6 5        116     74         0        0     25.6 0.201    30  0   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the data\n",
    "pima = read.csv(\"pima.txt\", sep=\"\\t\")\n",
    "# Here's a description of the data: https://rdrr.io/cran/faraway/man/pima.html\n",
    "head(pima)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. (a) Data Cleaning? What about Data Scrubbing? Data Sterilizing?\n",
    "\n",
    "This is a real data set, which means that there's likely going to be gaps and missing values in the data. Before doing any modeling, we should inspect the data and clean it if necesary.\n",
    "\n",
    "Perform simple graphical and numerical summaries of the data. Pay attention for missing or nonsensical values. Can you find any obvious irregularities? If so, take appropriate steps to correct these problems. In the markdown cell, specify what cleaning you did and why you did it.\n",
    "\n",
    "Finally, split your data into training and test sets. Let the training set contain $80\\%$ of the rows and the test set contain the remaining $20\\%$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    pregnant         glucose        diastolic         triceps     \n",
       " Min.   : 0.000   Min.   :  0.0   Min.   :  0.00   Min.   : 0.00  \n",
       " 1st Qu.: 1.000   1st Qu.: 99.0   1st Qu.: 62.00   1st Qu.: 0.00  \n",
       " Median : 3.000   Median :117.0   Median : 72.00   Median :23.00  \n",
       " Mean   : 3.845   Mean   :120.9   Mean   : 69.11   Mean   :20.54  \n",
       " 3rd Qu.: 6.000   3rd Qu.:140.2   3rd Qu.: 80.00   3rd Qu.:32.00  \n",
       " Max.   :17.000   Max.   :199.0   Max.   :122.00   Max.   :99.00  \n",
       "    insulin           bmi           diabetes           age       \n",
       " Min.   :  0.0   Min.   : 0.00   Min.   :0.0780   Min.   :21.00  \n",
       " 1st Qu.:  0.0   1st Qu.:27.30   1st Qu.:0.2437   1st Qu.:24.00  \n",
       " Median : 30.5   Median :32.00   Median :0.3725   Median :29.00  \n",
       " Mean   : 79.8   Mean   :31.99   Mean   :0.4719   Mean   :33.24  \n",
       " 3rd Qu.:127.2   3rd Qu.:36.60   3rd Qu.:0.6262   3rd Qu.:41.00  \n",
       " Max.   :846.0   Max.   :67.10   Max.   :2.4200   Max.   :81.00  \n",
       "      test      \n",
       " Min.   :0.000  \n",
       " 1st Qu.:0.000  \n",
       " Median :0.000  \n",
       " Mean   :0.349  \n",
       " 3rd Qu.:1.000  \n",
       " Max.   :1.000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get a summary of the data\n",
    "summary(pima)# Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>glucose</dt><dd>5</dd><dt>diastolic</dt><dd>35</dd><dt>triceps</dt><dd>227</dd><dt>insulin</dt><dd>374</dd><dt>bmi</dt><dd>11</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[glucose] 5\n",
       "\\item[diastolic] 35\n",
       "\\item[triceps] 227\n",
       "\\item[insulin] 374\n",
       "\\item[bmi] 11\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "glucose\n",
       ":   5diastolic\n",
       ":   35triceps\n",
       ":   227insulin\n",
       ":   374bmi\n",
       ":   11\n",
       "\n"
      ],
      "text/plain": [
       "  glucose diastolic   triceps   insulin       bmi \n",
       "        5        35       227       374        11 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check for zeros in columns where it doesn't make sense\n",
    "cols_with_zero <- c(\"glucose\", \"diastolic\", \"triceps\", \"insulin\", \"bmi\")\n",
    "sapply(pima[cols_with_zero], function(x) sum(x == 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>glucose</dt><dd>5</dd><dt>diastolic</dt><dd>35</dd><dt>triceps</dt><dd>227</dd><dt>insulin</dt><dd>374</dd><dt>bmi</dt><dd>11</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[glucose] 5\n",
       "\\item[diastolic] 35\n",
       "\\item[triceps] 227\n",
       "\\item[insulin] 374\n",
       "\\item[bmi] 11\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "glucose\n",
       ":   5diastolic\n",
       ":   35triceps\n",
       ":   227insulin\n",
       ":   374bmi\n",
       ":   11\n",
       "\n"
      ],
      "text/plain": [
       "  glucose diastolic   triceps   insulin       bmi \n",
       "        5        35       227       374        11 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Replace zeros with NA in the relevant columns\n",
    "pima[cols_with_zero] <- lapply(pima[cols_with_zero], function(x) ifelse(x == 0, NA, x))\n",
    "\n",
    "# Verify the changes\n",
    "sapply(pima[cols_with_zero], function(x) sum(is.na(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>glucose</dt><dd>0</dd><dt>diastolic</dt><dd>0</dd><dt>triceps</dt><dd>0</dd><dt>insulin</dt><dd>0</dd><dt>bmi</dt><dd>0</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[glucose] 0\n",
       "\\item[diastolic] 0\n",
       "\\item[triceps] 0\n",
       "\\item[insulin] 0\n",
       "\\item[bmi] 0\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "glucose\n",
       ":   0diastolic\n",
       ":   0triceps\n",
       ":   0insulin\n",
       ":   0bmi\n",
       ":   0\n",
       "\n"
      ],
      "text/plain": [
       "  glucose diastolic   triceps   insulin       bmi \n",
       "        0         0         0         0         0 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Impute missing values using the median\n",
    "library(dplyr)\n",
    "pima <- pima %>%\n",
    "  mutate(across(all_of(cols_with_zero), ~ ifelse(is.na(.), median(., na.rm = TRUE), .)))\n",
    "\n",
    "# Verify the imputation\n",
    "sapply(pima[cols_with_zero], function(x) sum(is.na(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "614"
      ],
      "text/latex": [
       "614"
      ],
      "text/markdown": [
       "614"
      ],
      "text/plain": [
       "[1] 614"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "154"
      ],
      "text/latex": [
       "154"
      ],
      "text/markdown": [
       "154"
      ],
      "text/plain": [
       "[1] 154"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "set.seed(123)\n",
    "\n",
    "# Split the data\n",
    "sample_index <- sample(1:nrow(pima), 0.8 * nrow(pima))\n",
    "train_data <- pima[sample_index, ]\n",
    "test_data <- pima[-sample_index, ]\n",
    "\n",
    "# Verify the split\n",
    "nrow(train_data)\n",
    "nrow(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of Cleaning Steps\n",
    "\n",
    "- Inspection: We inspected the data to understand its structure and identify potential issues.\n",
    "- Identifying Irregularities: We identified columns where zero values might be nonsensical.\n",
    "- Replacing Zeros with NA: We replaced zero values in specific columns with NA to indicate missing values.\n",
    "- Imputation: We used median imputation to handle missing values, which is a common method to preserve the central tendency without introducing bias.\n",
    "- Splitting Data: We split the data into training and test sets to prepare for model building and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. (b) Initial GLM modelling\n",
    "\n",
    "\n",
    "Our data is clean and we're ready to fit! What kind of model should we use to fit these data? Notice that the `test` variable is either $0$ or $1$, for whether the individual tested positive for diabetes. Because `test` is binary, we should use logistic regression (which is a kind of binomial regression).\n",
    "\n",
    "Fit a model with `test` as the response and all the other variables as predictors. Can you tell whether this model fits the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = test ~ pregnant + glucose + diastolic + triceps + \n",
       "    insulin + bmi + diabetes + age, family = binomial, data = train_data)\n",
       "\n",
       "Deviance Residuals: \n",
       "    Min       1Q   Median       3Q      Max  \n",
       "-2.5345  -0.7206  -0.4056   0.6950   2.3640  \n",
       "\n",
       "Coefficients:\n",
       "              Estimate Std. Error z value Pr(>|z|)    \n",
       "(Intercept) -8.9017449  0.8950353  -9.946  < 2e-16 ***\n",
       "pregnant     0.1184713  0.0363409   3.260  0.00111 ** \n",
       "glucose      0.0389511  0.0044634   8.727  < 2e-16 ***\n",
       "diastolic   -0.0120342  0.0096359  -1.249  0.21170    \n",
       "triceps      0.0001204  0.0144736   0.008  0.99336    \n",
       "insulin     -0.0010706  0.0012503  -0.856  0.39183    \n",
       "bmi          0.0945454  0.0200427   4.717 2.39e-06 ***\n",
       "diabetes     0.7121157  0.3205776   2.221  0.02633 *  \n",
       "age          0.0139045  0.0106135   1.310  0.19017    \n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 796.42  on 613  degrees of freedom\n",
       "Residual deviance: 570.09  on 605  degrees of freedom\n",
       "AIC: 588.09\n",
       "\n",
       "Number of Fisher Scoring iterations: 5\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit the logistic regression model\n",
    "glm_model <- glm(test ~ pregnant + glucose + diastolic + triceps + insulin + bmi + diabetes + age,\n",
    "                 data = train_data, family = binomial)\n",
    "\n",
    "# Summary of the model\n",
    "summary(glm_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model summary shows significant predictors (e.g., glucose, bmi, diabetes) with p-values less than 0.05 and a substantial reduction in deviance, the model can be considered to fit the data well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. (c) Remember Bayes\n",
    "\n",
    "A quick analytical interlude.\n",
    "\n",
    "Is diastolic blood pressure significant in the regression model? Do women who test positive have higher diastolic blood pressures? Explain the distinction between the two questions and discuss why the answers are only apparently contradictory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = test ~ pregnant + glucose + diastolic + triceps + \n",
       "    insulin + bmi + diabetes + age, family = binomial, data = train_data)\n",
       "\n",
       "Deviance Residuals: \n",
       "    Min       1Q   Median       3Q      Max  \n",
       "-2.5345  -0.7206  -0.4056   0.6950   2.3640  \n",
       "\n",
       "Coefficients:\n",
       "              Estimate Std. Error z value Pr(>|z|)    \n",
       "(Intercept) -8.9017449  0.8950353  -9.946  < 2e-16 ***\n",
       "pregnant     0.1184713  0.0363409   3.260  0.00111 ** \n",
       "glucose      0.0389511  0.0044634   8.727  < 2e-16 ***\n",
       "diastolic   -0.0120342  0.0096359  -1.249  0.21170    \n",
       "triceps      0.0001204  0.0144736   0.008  0.99336    \n",
       "insulin     -0.0010706  0.0012503  -0.856  0.39183    \n",
       "bmi          0.0945454  0.0200427   4.717 2.39e-06 ***\n",
       "diabetes     0.7121157  0.3205776   2.221  0.02633 *  \n",
       "age          0.0139045  0.0106135   1.310  0.19017    \n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 796.42  on 613  degrees of freedom\n",
       "Residual deviance: 570.09  on 605  degrees of freedom\n",
       "AIC: 588.09\n",
       "\n",
       "Number of Fisher Scoring iterations: 5\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the summary of the logistic regression model\n",
    "summary(glm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1: Is diastolic blood pressure significant in the regression model?\n",
    "\n",
    "To determine the significance of diastolic blood pressure in the regression model, refer to the p-value associated with the diastolic blood pressure coefficient in the logistic regression model summary. It is considered NOT statistically significant.\n",
    "\n",
    "Question 2: Do women who test positive have higher diastolic blood pressures?\n",
    "\n",
    "To address whether women who test positive have higher diastolic blood pressures, we can compare the average diastolic blood pressure for women who tested positive (test = 1) against those who tested negative (test = 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "74.4398148148148"
      ],
      "text/latex": [
       "74.4398148148148"
      ],
      "text/markdown": [
       "74.4398148148148"
      ],
      "text/plain": [
       "[1] 74.43981"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "70.5954773869347"
      ],
      "text/latex": [
       "70.5954773869347"
      ],
      "text/markdown": [
       "70.5954773869347"
      ],
      "text/plain": [
       "[1] 70.59548"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare diastolic blood pressure between positive and negative test groups\n",
    "mean_positive <- mean(train_data$diastolic[train_data$test == 1], na.rm = TRUE)\n",
    "mean_negative <- mean(train_data$diastolic[train_data$test == 0], na.rm = TRUE)\n",
    "mean_positive\n",
    "mean_negative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distinction and Discussion\n",
    "\n",
    "The two questions address different aspects:\n",
    "\n",
    "1. Significance in the Model: This examines whether diastolic blood pressure is a statistically significant predictor of diabetes in the context of the logistic regression model, considering all other predictors. This is a conditional analysis based on the presence of other variables in the model.\n",
    "\n",
    "2. Comparative Averages: This looks at the average diastolic blood pressure in isolation between the two groups (positive and negative test results). This is a marginal analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. (d) GLM Interpretation\n",
    "\n",
    "We've seen so many regression summaries up to this point, how is this one different from all the others? Well, to really understand any model, it can be helpful to loop back and plug the fitted results back into the model's mathematical form.\n",
    "\n",
    "Explicity write out the equation for the binomial regression model that you fit in (b). Then, in words, explain how a $1$ unit change of `glucose` affects `test`, assuming all other predictors are held constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\log\\left(\\frac{P(\\text{test}=1)}{1 - P(\\text{test}=1)}\\right) = \\beta_0 + \\beta_1 \\cdot \\text{pregnant} + \\beta_2 \\cdot \\text{glucose} + \\beta_3 \\cdot \\text{diastolic} + \\beta_4 \\cdot \\text{triceps} + \\beta_5 \\cdot \\text{insulin} + \\beta_6 \\cdot \\text{bmi} + \\beta_7 \\cdot \\text{diabetes} + \\beta_8 \\cdot \\text{age}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$\\beta_0$ is the intercept.\n",
    "\n",
    "$\\beta_1, \\beta_2, ...., \\beta_8$ are the coefficients for the predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In words, a one-unit increase in glucose (assuming all other predictors are held constant) results in a change in the log-odds of testing positive for diabetes by the amount of the coefficient of glucose. Specifically, if the coefficient of glucose ($\\beta_2$) is positive, it indicates that higher glucose levels are associated with a higher probability of testing positive for diabetes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. (e) GLM Prediction\n",
    "\n",
    "One of the downsides of Logistic Regression is that there isn't an easy way of evaulating the goodness of fit of the model without predicting on new data. But, if we have more data to test with, then there are many methods of evaluation to use. One of the best tools are confusion matrices, which (despite the name) are actually not that hard to understand.\n",
    "\n",
    "A confusion matrix compares the predicted outcomes of a Logistic Regression Model (or any classification model) with the actual classifications. For binary classification, it is a $2 \\times 2$ matrix where the rows are the models' predicted outcome and the columns are the actual classifications. An example is displayed below.\n",
    "\n",
    "|  | True | False |  \n",
    "| --- | --- | --- |\n",
    "| 1 | 103 | 37 |  \n",
    "| 0 | 55  | 64 |  \n",
    "\n",
    "In the example, we know the following information:\n",
    "* The [1,1] cell is the number of datapoints that were correctly predicted to be $1$. The value (103) is the number of True Positives (TP). \n",
    "* The [2,2] cell is the number of datapoints that were correctly predicted to be $0$. The value is the number of True Negatives (TN).\n",
    "* The [1, 2] cell is the number of datapoints that were predicted to be $1$ but where actually $0$. This is the number of False Positives (FP), also called Type I error. In the context of our diabetes dataset, this would mean our model predicted that the person would have diabetes, but they actually did not.\n",
    "* The [2, 1] cell is the number of datapoints that were predicted to be $0$ but where actually $1$. This is the number of False Negatives (FN), also called Type 2 error. In the context of our diabetes dataset, this would mean our model predicted that the person would not have diabetes, but they actually did have diabetes.\n",
    "\n",
    "Use your model to predict the outcomes of the test set. Then construct a confusion matrix for these predictions and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the probabilities on the test set\n",
    "test_pred_prob <- predict(glm_model, newdata = test_data, type = \"response\")\n",
    "\n",
    "# Convert probabilities to binary outcomes using 0.5 as the threshold\n",
    "test_pred <- ifelse(test_pred_prob > 0.5, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         Actual\n",
       "Predicted  0  1\n",
       "        0 90 23\n",
       "        1 12 29"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the confusion matrix\n",
    "confusion_matrix <- table(Predicted = test_pred, Actual = test_data$test)\n",
    "\n",
    "# Print the confusion matrix\n",
    "confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. (f) Evaluation Statistics\n",
    "\n",
    "Using the four values from the confusion matrix, we can construct evaulation statistics to get a numerical approximation for our model's performance. Spend some time researching accuracy, precision, recall and F score. \n",
    "\n",
    "Calculate these values for your model's predictions on the test set. Clearly display your results. How well do you think your model fits the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl>\n",
       "\t<dt>$accuracy</dt>\n",
       "\t\t<dd>0.772727272727273</dd>\n",
       "\t<dt>$precision</dt>\n",
       "\t\t<dd>0.707317073170732</dd>\n",
       "\t<dt>$recall</dt>\n",
       "\t\t<dd>0.557692307692308</dd>\n",
       "\t<dt>$f1_score</dt>\n",
       "\t\t<dd>0.623655913978495</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description}\n",
       "\\item[\\$accuracy] 0.772727272727273\n",
       "\\item[\\$precision] 0.707317073170732\n",
       "\\item[\\$recall] 0.557692307692308\n",
       "\\item[\\$f1\\_score] 0.623655913978495\n",
       "\\end{description}\n"
      ],
      "text/markdown": [
       "$accuracy\n",
       ":   0.772727272727273\n",
       "$precision\n",
       ":   0.707317073170732\n",
       "$recall\n",
       ":   0.557692307692308\n",
       "$f1_score\n",
       ":   0.623655913978495\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "$accuracy\n",
       "[1] 0.7727273\n",
       "\n",
       "$precision\n",
       "[1] 0.7073171\n",
       "\n",
       "$recall\n",
       "[1] 0.5576923\n",
       "\n",
       "$f1_score\n",
       "[1] 0.6236559\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate accuracy, precision, recall, and F1-score\n",
    "TP <- confusion_matrix[2, 2]\n",
    "TN <- confusion_matrix[1, 1]\n",
    "FP <- confusion_matrix[2, 1]\n",
    "FN <- confusion_matrix[1, 2]\n",
    "\n",
    "accuracy <- (TP + TN) / sum(confusion_matrix)\n",
    "precision <- TP / (TP + FP)\n",
    "recall <- TP / (TP + FN)\n",
    "f1_score <- 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Print the metrics\n",
    "list(accuracy = accuracy, precision = precision, recall = recall, f1_score = f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model shows a reasonable fit with an accuracy of about 77.27%. However, the recall value is relatively low, indicating that the model might be missing a considerable number of actual positive cases. This suggests a need for further refinement of the model, such as feature selection, tuning hyperparameters, or using more advanced modeling techniques to improve recall without compromising precision too much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. (g) Understanding Evaluation Statistics\n",
    "\n",
    "Answer the following questions in the markdown cell below.\n",
    "\n",
    "1. Give an example scenario for when accuracy would be a misleading evaulation statistic.\n",
    "2. Confusion matrices can also be used for non-binary classification problems. Describe what a confusion matrix would look like for a response with $3$ levels.\n",
    "3. You'll have to take our word on the fact (or spend some time researching) that Type I error and Type II error are inversely related. That is, if a model is very good at detecting false positives, then it will be bad at detecting false negatives. In the case of our diabetes dataset, would you prefer a model that overestimates the Type 1 error or overestimates the Type II error. Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Example Scenario for Misleading Accuracy\n",
    "\n",
    "Accuracy can be misleading in cases where the dataset is imbalanced, meaning one class is much more frequent than the other. For example, consider a medical diagnostic test for a rare disease that occurs in only 1% of the population. If the model simply predicts that no one has the disease, it will be 99% accurate because it correctly identifies the 99% who do not have the disease. However, it fails to identify any actual cases of the disease, which is a critical failure for a medical test.\n",
    "\n",
    "2. Confusion Matrix for a Response with 3 Levels\n",
    "\n",
    "For a response with three levels (e.g., Class A, Class B, Class C), the confusion matrix would be a 3x3 matrix:\n",
    "\n",
    "\\begin{array}{c|ccc}\n",
    "\\text{Predicted} & \\text{Actual Class A} & \\text{Actual Class B} & \\text{Actual Class C} \\\\\n",
    "\\hline\n",
    "\\text{Predicted A} & \\text{TP_A} & \\text{FP_B_A} & \\text{FP_C_A} \\\\\n",
    "\\text{Predicted B} & \\text{FP_A_B} & \\text{TP_B} & \\text{FP_C_B} \\\\\n",
    "\\text{Predicted C} & \\text{FP_A_C} & \\text{FP_B_C} & \\text{TP_C} \\\\\n",
    "\\end{array}\n",
    "\n",
    "Where:\n",
    "\n",
    "- TP_A, TP_B, TP_C: True Positives for Class A, B, and C respectively.\n",
    "- FP_B_A: False Positives where Class B is predicted as Class A.\n",
    "- FP_A_B: False Positives where Class A is predicted as Class B.\n",
    "- FP_C_A: False Positives where Class C is predicted as Class A.\n",
    "- And similarly for other entries.\n",
    "\n",
    "3. Preference Between Type I and Type II Error in Diabetes Dataset\n",
    "\n",
    "In the context of the diabetes dataset, preferring a model that overestimates Type I error (false positives) or Type II error (false negatives) depends on the consequences of these errors:\n",
    "\n",
    "- Type I Error (False Positive): The model predicts diabetes when the person does not have it. This could lead to unnecessary anxiety and medical tests for the individual.\n",
    "- Type II Error (False Negative): The model fails to predict diabetes when the person actually has it. This could lead to missed diagnoses and lack of necessary treatment, which can have serious health consequences.\n",
    "\n",
    "Given the severe health implications of untreated diabetes, it is generally more critical to avoid Type II errors. Therefore, it would be preferable to have a model that overestimates Type I error (false positives) rather than Type II error (false negatives). Ensuring that individuals who might have diabetes are correctly identified for further testing and potential treatment is more important than the inconvenience of false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. (h) Ethical Issues in Data Collection\n",
    "\n",
    "Read Maya Iskandarani's [piece](https://researchblog.duke.edu/2016/10/24/diabetes-and-privacy-meet-big-data/) on consent and privacy concerns raised by this dataset. Summarize those concerns here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maya Iskandarani's piece on consent and privacy concerns raised several issues regarding the Pima Indian Diabetes Dataset (PIDD):\n",
    "\n",
    "1. Informed Consent: The Pima tribe participated in a long-term observational study initiated by the NIH, originally meant to last 10 years but extended to 40 years. The complexity and duration of such studies make it challenging to provide participants with comprehensive information about how their data will be used in the future. This raises concerns about the adequacy of informed consent.\n",
    "\n",
    "2. Data Privacy: The PIDD has been publicly accessible for over two decades, containing sensitive personal information such as blood pressure, BMI, and number of pregnancies. While the dataset is valuable for refining machine learning algorithms to predict and prevent diabetes, the public availability of such detailed personal data poses significant privacy risks.\n",
    "\n",
    "3. Ethical Controversy: The long-term use and public accessibility of the PIDD highlight the ethical controversy around using historical and personal data without ongoing consent from the participants. Researchers cannot realistically inform participants about all future uses of their data, leading to concerns about \"eternal\" medical consent.\n",
    "\n",
    "4. Interdisciplinary Questions: The case of the Pima tribe illustrates broader interdisciplinary questions at the intersection of medical history, anthropology, bioethics, and data analytics. It challenges researchers to consider the long-term ethical implications of data collection and usage beyond immediate scientific goals.\n",
    "\n",
    "These concerns underscore the importance of establishing robust ethical guidelines and consent processes for the collection and use of personal data in medical research, particularly when such data is used for long-term studies and made publicly accessible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Practicing those Math skills\n",
    "\n",
    "One of the conditions of GLMs is that the \"random component\" of the data needs to come from the Exponential Family of Distributions. But how do we know if a distribution is in the Exponential Family? Well, we could look it up. Or we could be proper mathematicians and check the answer ourselves! Let's flex those math muscles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. (a) But it's in the name...\n",
    "\n",
    "Show that $Y \\sim exponential(\\lambda)$, where $\\lambda$ is known, is a member of the exponential family."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show that $( Y \\sim \\text{exponential}(\\lambda)$ belongs to the exponential family of distributions, we start with its probability density function (pdf):\n",
    "\n",
    "$ f(y; \\lambda) = \\lambda e^{-\\lambda y}, \\quad \\text{for } y \\geq 0 \\$\n",
    "\n",
    "The general form for a distribution in the exponential family is:\n",
    "\n",
    "$ f(y; \\theta) = h(y) \\exp \\left( \\frac{y \\theta - c(\\theta)}{\\phi} \\right)$\n",
    "\n",
    "Let's rewrite $( f(y; \\lambda) $ in this form:\n",
    "\n",
    "1. **Identify the components:**\n",
    "\n",
    "   - $( \\theta = \\lambda$\n",
    "   - $( h(y) = 1), because there's no additional function of \\( y \\) in the exponential term.$\n",
    "   - $( c(\\theta) = \\frac{1}{\\lambda}), derived from the moment generating function (MGF) of the exponential distribution.$\n",
    "   \n",
    "\n",
    "2. **Rewrite the pdf:**\n",
    "\n",
    "   The pdf $ f(y; \\lambda) = \\lambda e^{-\\lambda y}$ can be expressed as:\n",
    "\n",
    "   $ f(y; \\lambda) = 1 \\cdot e^{-\\lambda y} \\cdot \\lambda $\n",
    "\n",
    "   Here,\n",
    "   - ($ h(y) = 1 $\n",
    "   - $ \\frac{y \\theta - c(\\theta)}{\\phi} = -\\lambda y $\n",
    "\n",
    "   where $\\theta = \\lambda ), ( c(\\lambda) = \\frac{1}{\\lambda} ), and  \\phi = 1 $.\n",
    "\n",
    "Therefore, $ Y \\sim \\text{exponential}(\\lambda) $  can indeed be expressed in the form that characterizes the exponential family of distributions:\n",
    "\n",
    "$f(y; \\lambda) = \\lambda e^{-\\lambda y} = \\exp \\left( \\frac{y \\lambda - \\frac{1}{\\lambda}}{1} \\right) \\cdot 1 $\n",
    "\n",
    "Thus, $Y \\sim \\text{exponential}(\\lambda) $ belongs to the exponential family of distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. (b) Why can't plants do math? Because it gives them square roots!\n",
    "\n",
    "Let $Y_i \\sim exponential(\\lambda)$ where $i \\in \\{ 1, \\dots, n\\}$. Then $Z = \\sum_{i=1}^n Y_i \\sim Gamma(n, \\lambda)$. Show that $Z$ is also a member of the exponential family."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show that $Z = \\sum_{i=1}^n Y_i $ follows the form of the exponential family distribution, let's express it in a form that aligns with the general definition of the exponential family.\n",
    "\n",
    "Given:\n",
    "- $ Y_i \\sim \\text{Exponential}(\\lambda) $, with probability density function $ f_{Y_i}(y_i; \\lambda) = \\lambda e^{-\\lambda y_i}, \\quad y_i \\geq 0 $.\n",
    "- $ Z = \\sum_{i=1}^n Y_i $.\n",
    "\n",
    "First, the joint distribution of $ Y_1, \\dots, Y_n $:\n",
    "\n",
    "$f_{Y_1,\\dots,Y_n}(y_1,\\dots,y_n; \\lambda) = \\prod_{i=1}^n \\lambda e^{-\\lambda y_i} = \\lambda^n e^{-\\lambda \\sum_{i=1}^n y_i}.$\n",
    "\n",
    "Now, the distribution of \\( Z \\):\n",
    "\n",
    "$f_Z(z; \\lambda) = \\int_{y_1+\\cdots+y_n=z} \\lambda^n e^{-\\lambda z} \\, dy_1 \\cdots dy_n.$\n",
    "\n",
    "To express $ f_Z(z; \\lambda) $ in the form of an exponential family distribution, consider:\n",
    "$f_Z(z; \\lambda) = \\lambda^n e^{-\\lambda z} \\cdot \\mathbf{1}\\{z \\geq 0\\}.$\n",
    "\n",
    "This can be written as:\n",
    "$f_Z(z; \\lambda) = \\exp \\left( n \\log \\lambda - \\lambda z \\right) \\cdot \\mathbf{1}\\{z \\geq 0\\}.$\n",
    "\n",
    "Therefore, $ Z \\sim \\text{Gamma}(n, \\lambda) $ can be expressed in the exponential family form:\n",
    "$f_Z(z; \\lambda) = \\exp \\left( \\frac{z \\cdot \\theta - b(\\theta)}{a(\\phi)} + c(z, \\phi) \\right),$\n",
    "\n",
    "where:\n",
    "- $ \\theta = \\lambda ,$\n",
    "- $ b(\\theta) = n \\log \\lambda ,$\n",
    "- $ a(\\phi) = 1 ,$\n",
    "- $ c(z, \\phi) = 0.$\n",
    "\n",
    "Thus, $ Z $ belongs to the exponential family with parameters $\\theta = \\lambda , \\phi = 1 $, sufficient statistic $ T(z) = z $, natural parameter $\\eta = \\log \\lambda $, and log-partition function $ A(\\eta) = -n \\log \\lambda $.\n",
    "\n",
    "Therefore, we have shown that $ Z = \\sum_{i=1}^n Y_i \\sim \\text{Gamma}(n, \\lambda) $ is indeed a member of the exponential family."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
